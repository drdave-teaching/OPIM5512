{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p4Hfmn5ZwLOC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# My Second Agent: Parse LPs (v0.2.0)"
      ],
      "metadata": {
        "id": "xP85HIx4jksS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dr. Dave Wanik - Operations and Information Management - University of Connecticut**"
      ],
      "metadata": {
        "id": "dPErVJcno4fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So our first agent was to just write a linear programming problem - this agent works to parse a word problem into its main elements: objective function, decision variables, constraints. Then it stacks them in for loops and solves all possible combinations and shows the output."
      ],
      "metadata": {
        "id": "dTKzHWfSoklE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ› ï¸ Add an `lp_parser` sub-package to *agent-toolkit*\n",
        "\n",
        "### 1 ï¸âƒ£  Create the folder structure\n",
        "\n",
        "```\n",
        "agent-toolkit/               â† repo root (already exists)\n",
        "â””â”€â”€ src/\n",
        "    â”œâ”€â”€ agent_toolkit/       â† existing code\n",
        "    â””â”€â”€ lp_parser/           â† NEW\n",
        "        â”œâ”€â”€ __init__.py\n",
        "        â””â”€â”€ core.py\n",
        "```\n",
        "\n",
        "> **How:**  \n",
        "> *In VS Code Explorer â†’ Right-click `src/` â†’ â€œNew Folderâ€ â†’ name it `lp_parser`, then add both files.*\n",
        "\n",
        "---\n",
        "\n",
        "### 2 ï¸âƒ£  Fill the files\n",
        "\n",
        "**`src/lp_parser/__init__.py`**\n",
        "\n",
        "```python\n",
        "from .core import parse_word_problem\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**`src/lp_parser/core.py`**\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Natural-language âœ structured JSON extractor for small LP word problems.\n",
        "Scope (v0): max-profit LPs with â€˜â‰¤â€™ constraints and integer, non-negative variables.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from openai import OpenAI\n",
        "\n",
        "_JSON_SCHEMA = \"\"\"\n",
        "{\n",
        "  \"objective\": {\"sense\": \"max\", \"coeff\": {\"<var>\": <float>, ...}},\n",
        "  \"vars\": {\"<var>\": {\"ub\": <int>}, ...},\n",
        "  \"constraints\": [\n",
        "    {\"name\": \"<string>\", \"coeff\": {\"<var>\": <float>, ...}, \"rhs\": <float>},\n",
        "    ...\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def _llm_extract(problem_text: str, model: str = \"gpt-4o\") -> str:\n",
        "    \"\"\"Call OpenAI with JSON mode and return raw JSON string.\"\"\"\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"Extract the linear-program parameters ONLY as valid JSON \"\n",
        "                    f\"matching this schema (no extra keys): {_JSON_SCHEMA}\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": problem_text},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def parse_word_problem(problem_text: str, model: str = \"gpt-4o\") -> dict:\n",
        "    \"\"\"Return a Python dict describing the LP or raise ValueError on failure.\"\"\"\n",
        "    raw = _llm_extract(problem_text, model=model)\n",
        "    try:\n",
        "        lp = json.loads(raw)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"LLM did not return valid JSON: {e}\") from None\n",
        "    # â¬‡ï¸ very light validation (students can improve)\n",
        "    if \"objective\" not in lp or \"vars\" not in lp or \"constraints\" not in lp:\n",
        "        raise ValueError(\"Parsed JSON missing required keys.\")\n",
        "    return lp\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3 ï¸âƒ£  Wire this new parser into the main package\n",
        "\n",
        "**Edit** `src/agent_toolkit/__init__.py` (existing):\n",
        "\n",
        "```python\n",
        "from .core import run_chat_agent\n",
        "from brute_force_lp.core import brute_force_lp, sample_lp\n",
        "from lp_parser.core import parse_word_problem          # â† NEW\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4 ï¸âƒ£  Add an example notebook (optional)\n",
        "\n",
        "1. Create `examples/parse_and_solve.ipynb`\n",
        "2. Demonstrate:\n",
        "\n",
        "```python\n",
        "from lp_parser import parse_word_problem\n",
        "from agent_toolkit import brute_force_lp\n",
        "\n",
        "prompt = \"\"\"Veerman Furniture Company makes three kinds of office furniture ...\"\"\"\n",
        "lp = parse_word_problem(prompt)\n",
        "best, con_df = brute_force_lp(lp)\n",
        "print(best)\n",
        "con_df\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5 ï¸âƒ£  Update `pyproject.toml` dependencies\n",
        "\n",
        "Add **`openai`** (already there) plus **`jsonschema`** later if you want strict validation.\n",
        "\n",
        "```toml\n",
        "dependencies = [\n",
        "    \"openai>=1.0.0\",\n",
        "    \"pandas\",\n",
        "    \"numpy\"\n",
        "]\n",
        "```\n",
        "\n",
        "*(No version bump needed if youâ€™re still at 0.2.0â€”but feel free.)*\n",
        "\n",
        "---\n",
        "\n",
        "### 6 ï¸âƒ£  Commit & push\n",
        "\n",
        "```bash\n",
        "git add -A\n",
        "git commit -m \"v0.2.0 â€“ add lp_parser sub-package (word-problem âœ JSON)\"\n",
        "git push origin main\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7 ï¸âƒ£  Test in Colab\n",
        "\n",
        "Use this to test your installation and confirm that `parse_word_problem()` returns structured JSON from a word problem.\n",
        "\n",
        "```python\n",
        "!pip install --force-reinstall -q git+https://github.com/<your-username>/agent-toolkit.git\n",
        "\n",
        "from agent_toolkit import parse_word_problem\n",
        "\n",
        "# Example word problem (Veerman Furniture)\n",
        "problem = \"\"\"\n",
        "Veerman Furniture Company makes three kinds of office furniture: chairs, desks, and tables.\n",
        "Each product requires labor in fabrication, assembly, and shipping departments. The available\n",
        "hours per department are: 1850 (fabrication), 2400 (assembly), and 1500 (shipping).\n",
        "Demand limits are: 360 chairs, 300 desks, 100 tables.\n",
        "Profit per product is: $15 (chair), $24 (desk), and $18 (table).\n",
        "Fabrication uses 4/6/2 hours per chair/desk/table,\n",
        "Assembly uses 3/5/7 hours,\n",
        "Shipping uses 3/2/4 hours.\n",
        "\"\"\"\n",
        "\n",
        "# Run the parser\n",
        "parsed_lp = parse_word_problem(problem)\n",
        "parsed_lp\n",
        "```\n",
        "\n",
        "If working correctly, this should output a dictionary with:\n",
        "- `\"objective\"` (sense and profit coefficients)\n",
        "- `\"vars\"` (decision variable bounds)\n",
        "- `\"constraints\"` (each constraintâ€™s coefficients and RHS)\n",
        "\n",
        "Once the solver is added later, weâ€™ll plug this JSON into `brute_force_lp()` or `sample_lp()`.\n"
      ],
      "metadata": {
        "id": "WnWxGiD7nh7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try out the agent!"
      ],
      "metadata": {
        "id": "VtB4WPUgu9nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old version (v0.1.0)"
      ],
      "metadata": {
        "id": "p4Hfmn5ZwLOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --force-reinstall --no-cache-dir git+https://github.com/drdww/agent-toolkit.git@v0.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "su8NtGjPu_xi",
        "outputId": "b240bf3b-8c7e-4e99-d136-38ac4de3f3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/drdww/agent-toolkit.git@v0.1.0\n",
            "  Cloning https://github.com/drdww/agent-toolkit.git (to revision v0.1.0) to /tmp/pip-req-build-db83b11r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/drdww/agent-toolkit.git /tmp/pip-req-build-db83b11r\n",
            "  Running command git checkout -q 96b2069a591b5f153dcb8010eb6da14036c63ff3\n",
            "  Resolved https://github.com/drdww/agent-toolkit.git to commit 96b2069a591b5f153dcb8010eb6da14036c63ff3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai>=1.0.0 (from agent-toolkit==0.1.0)\n",
            "  Downloading openai-1.93.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m185.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<5,>=4.11 (from openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi (from httpx<1,>=0.23.0->openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai>=1.0.0->agent-toolkit==0.1.0)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading openai-1.93.0-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m755.0/755.0 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m154.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m159.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m163.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m161.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.7/157.7 kB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: agent-toolkit\n",
            "  Building wheel for agent-toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for agent-toolkit: filename=agent_toolkit-0.1.0-py3-none-any.whl size=1767 sha256=331243242db93fd5594fa8057505bf954acfe5bea09347f97f3d36d071faccdb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9gtv6i_p/wheels/af/07/4e/9cbbb784ac8195006349dd53f36e58e63ff6a2c3e0f312809d\n",
            "Successfully built agent-toolkit\n",
            "Installing collected packages: typing-extensions, tqdm, sniffio, jiter, idna, h11, distro, certifi, annotated-types, typing-inspection, pydantic-core, httpcore, anyio, pydantic, httpx, openai, agent-toolkit\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: sniffio\n",
            "    Found existing installation: sniffio 1.3.1\n",
            "    Uninstalling sniffio-1.3.1:\n",
            "      Successfully uninstalled sniffio-1.3.1\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.10.0\n",
            "    Uninstalling jiter-0.10.0:\n",
            "      Successfully uninstalled jiter-0.10.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: distro\n",
            "    Found existing installation: distro 1.9.0\n",
            "    Uninstalling distro-1.9.0:\n",
            "      Successfully uninstalled distro-1.9.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.6.15\n",
            "    Uninstalling certifi-2025.6.15:\n",
            "      Successfully uninstalled certifi-2025.6.15\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: typing-inspection\n",
            "    Found existing installation: typing-inspection 0.4.1\n",
            "    Uninstalling typing-inspection-0.4.1:\n",
            "      Successfully uninstalled typing-inspection-0.4.1\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.9.0\n",
            "    Uninstalling anyio-4.9.0:\n",
            "      Successfully uninstalled anyio-4.9.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.93.0\n",
            "    Uninstalling openai-1.93.0:\n",
            "      Successfully uninstalled openai-1.93.0\n",
            "  Attempting uninstall: agent-toolkit\n",
            "    Found existing installation: agent-toolkit 0.1.0\n",
            "    Uninstalling agent-toolkit-0.1.0:\n",
            "      Successfully uninstalled agent-toolkit-0.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed agent-toolkit-0.1.0 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.6.15 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 openai-1.93.0 pydantic-2.11.7 pydantic-core-2.33.2 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.14.0 typing-inspection-0.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              },
              "id": "a17536e5c3ae4345887880faefb02ca3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The new version (v0.2.0)"
      ],
      "metadata": {
        "id": "FsMSUFPewxND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Later, upgrade to the parser version\n",
        "# # It is installed like a package! You didn't clone!\n",
        "# !pip install --quiet --upgrade git+https://github.com/drdww/agent-toolkit.git@v0.2.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmfSp6B3wQl-",
        "outputId": "5261b8db-b965-4e0f-aa5d-700208e173e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for agent-toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â¬‡ï¸ 1.  Always start by installing the exact version you need\n",
        "!pip install --quiet --force-reinstall --no-cache-dir \\\n",
        "    git+https://github.com/drdww/agent-toolkit.git@v0.2.0\n",
        "\n",
        "# â¬‡ï¸ 2.  Securely load your OpenAI key for this Colab session\n",
        "import os, getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Paste your OpenAI key â†’ \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQUQxxH5wQk6",
        "outputId": "9537ae39-7df3-4441-a290-5f7731342a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m207.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m168.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m755.0/755.0 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m160.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m185.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m146.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m207.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m194.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m188.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m196.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m206.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m387.0/387.0 kB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m170.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.7/157.7 kB\u001b[0m \u001b[31m182.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for agent-toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mPaste your OpenAI key â†’ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â¬‡ï¸ 3.  Import the parser from agent-toolkit v0.2.0\n",
        "from agent_toolkit import parse_word_problem\n",
        "import json, pprint\n",
        "\n",
        "problem = \"\"\"\n",
        "Veerman Furniture Company makes three kinds of office furniture: chairs, desks, and tables.\n",
        "Each product requires labor in fabrication (4,6,2), assembly (3,5,7), and shipping (3,2,4).\n",
        "Available hours: 1850 fabrication, 2400 assembly, 1500 shipping.\n",
        "Demand limits: 360 chairs, 300 desks, 100 tables.\n",
        "Profit per unit: $15 chairs, $24 desks, $18 tables.\n",
        "Goal: maximize total profit for the coming quarter.\n",
        "\"\"\"\n",
        "\n",
        "lp_json = parse_word_problem(problem)\n",
        "\n",
        "print(\"ğŸ¯ Parsed LP:\")\n",
        "pprint.pp(lp_json, width=100, compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FObEfFnxqa7",
        "outputId": "38c4f330-3d55-4a40-cc86-e519a476321e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Parsed LP:\n",
            "{'objective': {'sense': 'max', 'coeff': {'chairs': 15.0, 'desks': 24.0, 'tables': 18.0}},\n",
            " 'vars': {'chairs': {'ub': 360}, 'desks': {'ub': 300}, 'tables': {'ub': 100}},\n",
            " 'constraints': [{'name': 'fabrication',\n",
            "                  'coeff': {'chairs': 4.0, 'desks': 6.0, 'tables': 2.0},\n",
            "                  'rhs': 1850.0},\n",
            "                 {'name': 'assembly',\n",
            "                  'coeff': {'chairs': 3.0, 'desks': 5.0, 'tables': 7.0},\n",
            "                  'rhs': 2400.0},\n",
            "                 {'name': 'shipping',\n",
            "                  'coeff': {'chairs': 3.0, 'desks': 2.0, 'tables': 4.0},\n",
            "                  'rhs': 1500.0}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ‰ Nice work!\n",
        "\n",
        "Youâ€™ve just built **Layer 1**: a reusable `lp_parser` module that converts natural-language word problems into structured JSON that can be used by a solver.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§­ Next Steps (coming soon...)\n",
        "\n",
        "We'll build **Layer 2**: a solver that can take your JSON and return:\n",
        "\n",
        "- âœ… The optimal values of the decision variables\n",
        "- âœ… The maximum (or minimum) objective value\n",
        "- âœ… A summary report of how the solution uses each constraint (LHS vs RHS)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§° Once you have both layers, youâ€™ll be able to:\n",
        "\n",
        "- ğŸ§  Use GPT to extract structured LPs from real-world problem descriptions\n",
        "- ğŸ§® Solve them using a brute-force or Monte Carlo engine\n",
        "- ğŸ§¾ Generate readable reports (ideal for teaching and debugging)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ Future enhancements (stretch goals)\n",
        "\n",
        "- ğŸ”„ Swap out the brute-force solver for PuLP or OR-Tools\n",
        "- ğŸ¯ Add support for equality (`=`) and â‰¥ constraints\n",
        "- ğŸ“Š Visualize the feasible region or sample distribution\n",
        "- ğŸ¤– Build an agent-style wrapper (`solve_lp_from_prompt()`) that combines everything\n",
        "\n",
        "---\n",
        "\n",
        "Stay tuned for the next notebook â€” youâ€™re halfway to a full natural languageâ€“toâ€“solver pipeline!\n"
      ],
      "metadata": {
        "id": "_ViS03ysuhHP"
      }
    }
  ]
}